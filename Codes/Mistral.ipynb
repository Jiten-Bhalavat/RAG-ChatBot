{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "# from dotenv import load_dotenv, find_dotenv\n",
    "# _ = load_dotenv(find_dotenv()) # read local .env file\n",
    " \n",
    "openai.api_key  = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llama-index qdrant-client \n",
    "# !pip install pdfminer.six llama-index-vector-stores-qdrant\n",
    "# !pip install llama-index-llms-openai llama-index-embeddings-jinaai \n",
    "# !pip install llama-index-llms-huggingface \"huggingface_hub[inference]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_inference_api_key = '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'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Promt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "qa_prompt_tmpl = (\n",
    "    \"Context information is below.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Given the context information and not prior knowledge, \"\n",
    "    \"answer the query.\\n\"\n",
    "    \"If the context information does not contain an answer to the query, \"\n",
    "    \"respond with \\\"No information\\\".\\n\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "qa_prompt = PromptTemplate(qa_prompt_tmpl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPEN SOURCE LLM MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.huggingface import HuggingFaceInferenceAPI\n",
    "\n",
    "mixtral_llm = HuggingFaceInferenceAPI(\n",
    "    model_name=\"mistralai/Mixtral-8x7B-Instruct-v0.1\", \n",
    "    token=hf_inference_api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x0000015F0F3B5550> system_prompt=None messages_to_prompt=<function messages_to_prompt at 0x0000015F02E7FA60> completion_to_prompt=<function default_completion_to_prompt at 0x0000015F02EE2AC0> output_parser=None pydantic_program_mode=<PydanticProgramMode.DEFAULT: 'default'> query_wrapper_prompt=None model_name='mistralai/Mixtral-8x7B-Instruct-v0.1' token='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' timeout=None headers=None cookies=None task=None context_window=3900 num_output=256 is_chat_model=False is_function_calling_model=False\n"
     ]
    }
   ],
   "source": [
    "print(mixtral_llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open Source Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "jina_emb_api_key = \"jina_c8ccbf103ade414fa7c6d87acf371d74zeCtDLC0NWQ6kuAF3Wwgc0pk5mgg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.jinaai import JinaEmbedding\n",
    "\n",
    "jina_embedding_model = JinaEmbedding(\n",
    "    api_key=jina_emb_api_key,\n",
    "    model=\"jina-embeddings-v2-base-en\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "\n",
    "pdf_data= SimpleDirectoryReader(input_files=[r\"C:\\Users\\ASUS\\Desktop\\Jiten\\Hybrid Search\\DATA\\pdf testing.pdf\"]).load_data()\n",
    "print(len(pdf_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant_api_key = \"3iWyjvmaxwb1y7rSpmq73siVjXHiMpmpAX0fwHnN6XPdaoYg08ahSw\"\n",
    "qdrant_server = \"https://28b8c975-f097-4f6f-9604-ad8e9fd4521f.us-east4-0.gcp.cloud.qdrant.io:6333\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import qdrant_client\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "\n",
    "client = qdrant_client.QdrantClient(qdrant_server, api_key=qdrant_api_key)\n",
    "vector_store = QdrantVectorStore(client=client, collection_name=\"pdftest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_26408\\4116570800.py:11: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  service_context = ServiceContext.from_defaults(\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.storage.storage_context import StorageContext\n",
    "from llama_index.core import (\n",
    "\t\tVectorStoreIndex,\n",
    "\t\tServiceContext,\n",
    "\t\tget_response_synthesizer,\n",
    ")\n",
    "\n",
    "# set up the service and storage contexts\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=mixtral_llm, embed_model=jina_embedding_model\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "# create an index\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    pdf_data, storage_context=storage_context, service_context=service_context\n",
    ")\n",
    "\n",
    "# configure retriever\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=5,\n",
    ")\n",
    "\n",
    "# configure response synthesizer\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    service_context=service_context,\n",
    "    text_qa_template=qa_prompt,\n",
    "    response_mode=\"compact\",\n",
    ")\n",
    "\n",
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The existing algorithms used for time series prediction are ARIMA/SARIMA, Facebook Prophet, Logistic Regression, Random Forest, SVM (Support Vector Machine), LSTM (Long Short-Term Memory), GRU (Gated Recurrent Unit), Temporal Fusion Transformer (Google), XGBoost, AdaBoost, CATBoost, Neural Prophet, and DeepAR (Amazon).\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"\"\"\n",
    "Which are the existing algorithms used for time series prediction \n",
    "\"\"\")\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
