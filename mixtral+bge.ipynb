{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        Models Used:  \n",
    "1.) LLM = Mixtral \n",
    "2.) Embeddings = BAAI/bge-large-en-v1.5  \n",
    "3.) Re-Ranker = BAAI/bge-reranker-large      \n",
    "\n",
    "        Steps :     \n",
    "1.) Loading the data   \n",
    "2.) Creating the nodes   \n",
    "3.) HuggingFace Embeddings and LLM Model( Opensource as well) --> Service Context  \n",
    "4.) Sentence Indexing and Storing it for further use  \n",
    "5.) Re-Ranking (Bge-Large)  \n",
    "6.) Query Engine with Re-Ranking  \n",
    "7.) Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "# from dotenv import load_dotenv, find_dotenv\n",
    "# _ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "openai.api_key  = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llama-index-llms-huggingface \"huggingface_hub[inference]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "\n",
    "pdf_data2= SimpleDirectoryReader(input_files=[r\"C:\\Users\\ASUS\\Desktop\\Jiten\\Hybrid Search\\DATA\\pdf testing.pdf\"]).load_data()\n",
    "print(len(pdf_data2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.) Creating the Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.text_splitter import SentenceSplitter\n",
    "\n",
    "def create_nodes(documents, chunk_size=500, chunk_overlap=100):\n",
    "  node_parser = SentenceSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "  nodes = node_parser.get_nodes_from_documents(documents)\n",
    "  return nodes\n",
    "\n",
    "nodes1 = create_nodes(pdf_data2)\n",
    "# print(len(nodes1)) # 29"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HuggingFace Model and Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "qa_prompt_tmpl = (\n",
    "    \"Context information is below.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Given the context information and not prior knowledge, \"\n",
    "    \"answer the query.\\n\"\n",
    "    \"If the context information does not contain an answer to the query, \"\n",
    "    \"respond with \\\"No information\\\".\\n\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "qa_prompt = PromptTemplate(qa_prompt_tmpl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_inference_api_key = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LLM=Mistral-8x7b\n",
    "\n",
    "from llama_index.llms.huggingface import HuggingFaceInferenceAPI\n",
    "\n",
    "mixtral_llm = HuggingFaceInferenceAPI(\n",
    "    model_name=\"mistralai/Mixtral-8x7B-Instruct-v0.1\", \n",
    "    token=hf_inference_api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_11276\\1261285089.py:5: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  service_context = ServiceContext.from_defaults(\n"
     ]
    }
   ],
   "source": [
    "## Embeddings\n",
    "\n",
    "from llama_index.core import ServiceContext\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=mixtral_llm,\n",
    "    embed_model=\"local:BAAI/bge-large-en-v1.5\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-Ranking using bge-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.core.indices.postprocessor import SentenceTransformerRerank\n",
    "\n",
    "from llama_index.core.postprocessor.sbert_rerank import SentenceTransformerRerank\n",
    "bge_reranker_large= SentenceTransformerRerank(model=\"BAAI/bge-reranker-large\", top_n=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core import (\n",
    "\t\tVectorStoreIndex,\n",
    "\t\tget_response_synthesizer,\n",
    ")\n",
    "\n",
    "sentence_index = VectorStoreIndex(nodes1, service_context=service_context)\n",
    "\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=sentence_index,\n",
    "    similarity_top_k=5,\n",
    ")\n",
    "\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    service_context=service_context,\n",
    "    text_qa_template=qa_prompt,\n",
    "    response_mode=\"compact\",\n",
    ")\n",
    "\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    node_postprocessors=[bge_reranker_large]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
